{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1620.  , 1620.  ,   19.73, ..., 1603.8 ,    5.  ,   92.  ],\n",
       "       [2160.  , 2160.  ,    5.33, ...,    0.  ,    0.  ,    0.  ],\n",
       "       [2160.  , 2160.  ,    5.33, ...,    0.  ,    0.  ,  388.  ],\n",
       "       ...,\n",
       "       [2160.  , 2160.  ,    6.14, ...,    0.  ,    0.  ,    0.  ],\n",
       "       [1620.  , 1620.  ,    5.33, ...,  615.6 ,    0.  ,   90.  ],\n",
       "       [1674.  , 3348.  ,    5.33, ...,    0.  ,    0.  ,    0.  ]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "raw_csv_data=np.loadtxt(' Audiobooks_data.csv',delimiter=',')\n",
    "unscaled_inputs_all=load_data[:,1:-1]\n",
    "targets_all=load_data[:,-1]\n",
    "unscaled_inputs_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#balancing the dataset\n",
    "num_one_targets=int(np.sum(targets_all))\n",
    "zero_targets_counter=0\n",
    "indices_to_remove=[]\n",
    "\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i]==0:\n",
    "        zero_targets_counter+=1\n",
    "        if zero_targets_counter>num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "unscaled_input_equal_priors=np.delete(unscaled_inputs_all,indices_to_remove,axis=0)\n",
    "targets_equal_priors=np.delete(targets_all,indices_to_remove,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize the input\n",
    "scaled_inputs=preprocessing.scale(unscaled_input_equal_priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle the data\n",
    "shuffled_indices=np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "shuffled_inputs=scaled_inputs[shuffled_indices]\n",
    "shuffled_targets=target_equal_priors[shuffled_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the dataset into train ,validation and test \n",
    "samples_count=shuffled_inputs.shape[0]\n",
    "train_samples_count=int(0.8*samples_count)\n",
    "test_samples_count=int(0.1*samples_count)\n",
    "validation_samples_count=int(0.1*samples_count)\n",
    "\n",
    "train_inputs=shuffled_inputs[:train_samples_count]\n",
    "train_targets=shuffled_targets[:train_samples_count]\n",
    "\n",
    "validation_inputs=shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets=shuffled_targets[:train_samples_count+validation_samples_count]\n",
    "\n",
    "\n",
    "test_inputs=shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "test_targets=shuffled_targets[:train_samples_count+validation_samples_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the three datasets in/npz\n",
    "np.savez('Audiobooks_data_train',inputs=train_inputs,targets=train_targets)\n",
    "np.savez('Audiobooks_data_validation',inputs=validation_inputs,targets=validation_targets)\n",
    "np.savez('Audiobooks_data_test',inputs=test_inputs,targets=test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making class for batching\n",
    "import numpy as np \n",
    "class Audiobooks_Data_Reader:\n",
    "    def __init__(self,dataset,batch_size=None):\n",
    "        #load the train ,test,validation data\n",
    "        npz=np.load('Audiobooks_data_{0}.npz'.format(dataset))\n",
    "        #giving value to the inouts as float and target as interger\n",
    "        self.inputs,self.targets=npz['inputs'].astype(np.float),npz['targets'].astype(np.int)\n",
    "        if batch_size is None:\n",
    "            self.batch_size=self.inputs.shape[0]\n",
    "        else:\n",
    "             self.batch_size=batch_size\n",
    "        self.curr_batch=0\n",
    "        self.batch_count=self.inputs.shape[0]//self.batch_size\n",
    "    \n",
    "    def __next__(self):\n",
    "         if self.curr_batch>=self.batch_count:\n",
    "                self.curr_batch=0\n",
    "                raise StopIteration()\n",
    "                raise StopIteration()\n",
    "         batch_slice=slice(self.curr_batch*self.batch_size,(self.curr_batch+1)*self.batch_size)\n",
    "         input_batch=self.inputs[batch_slice]\n",
    "         targets_batch=self.targets[batch_slice]\n",
    "         self.curr_batch+=1\n",
    "        \n",
    "         classes_num=2\n",
    "         targets_one_hot=np.zeros((targets_batch.shape[0],classes_num))\n",
    "         targets_one_hot[range(targets_batch.shape[0]),targets_batch]=1\n",
    "        \n",
    "         return input_batch,targets_one_hot\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1.Training loss:0.017.validation loss:0.560.validaiton accuracy :80.31%\n",
      "Epoch1.Training loss:0.016.validation loss:0.545.validaiton accuracy :80.31%\n",
      "Epoch1.Training loss:0.014.validation loss:0.533.validaiton accuracy :80.31%\n",
      "Epoch1.Training loss:0.015.validation loss:0.526.validaiton accuracy :80.31%\n",
      "Epoch1.Training loss:0.015.validation loss:0.524.validaiton accuracy :80.31%\n",
      "Epoch1.Training loss:0.016.validation loss:0.523.validaiton accuracy :80.31%\n",
      "Epoch1.Training loss:0.015.validation loss:0.524.validaiton accuracy :80.31%\n",
      "Epoch2.Training loss:0.018.validation loss:0.527.validaiton accuracy :80.31%\n",
      "Epoch3.Training loss:0.014.validation loss:0.530.validaiton accuracy :80.31%\n",
      "Epoch4.Training loss:0.015.validation loss:0.532.validaiton accuracy :80.31%\n",
      "Epoch5.Training loss:0.015.validation loss:0.535.validaiton accuracy :80.31%\n",
      "Epoch6.Training loss:0.015.validation loss:0.537.validaiton accuracy :80.31%\n",
      "Epoch7.Training loss:0.012.validation loss:0.539.validaiton accuracy :80.31%\n",
      "Epoch8.Training loss:0.012.validation loss:0.541.validaiton accuracy :80.31%\n",
      "Epoch9.Training loss:0.013.validation loss:0.543.validaiton accuracy :80.31%\n",
      "Epoch10.Training loss:0.013.validation loss:0.545.validaiton accuracy :80.31%\n",
      "Epoch11.Training loss:0.012.validation loss:0.547.validaiton accuracy :80.31%\n",
      "Epoch12.Training loss:0.015.validation loss:0.549.validaiton accuracy :80.31%\n",
      "Epoch13.Training loss:0.015.validation loss:0.550.validaiton accuracy :80.31%\n",
      "Epoch14.Training loss:0.015.validation loss:0.551.validaiton accuracy :80.31%\n",
      "Epoch15.Training loss:0.012.validation loss:0.552.validaiton accuracy :80.09%\n",
      "Epoch16.Training loss:0.016.validation loss:0.553.validaiton accuracy :79.87%\n",
      "Epoch17.Training loss:0.013.validation loss:0.554.validaiton accuracy :79.87%\n",
      "Epoch18.Training loss:0.010.validation loss:0.555.validaiton accuracy :79.87%\n",
      "Epoch19.Training loss:0.015.validation loss:0.557.validaiton accuracy :79.87%\n",
      "Epoch20.Training loss:0.013.validation loss:0.558.validaiton accuracy :79.87%\n",
      "Epoch21.Training loss:0.013.validation loss:0.560.validaiton accuracy :79.87%\n",
      "Epoch22.Training loss:0.011.validation loss:0.562.validaiton accuracy :79.87%\n",
      "Epoch23.Training loss:0.012.validation loss:0.563.validaiton accuracy :79.64%\n",
      "Epoch24.Training loss:0.014.validation loss:0.565.validaiton accuracy :79.64%\n",
      "Epoch25.Training loss:0.014.validation loss:0.567.validaiton accuracy :79.64%\n",
      "Epoch26.Training loss:0.012.validation loss:0.569.validaiton accuracy :79.64%\n",
      "Epoch27.Training loss:0.014.validation loss:0.571.validaiton accuracy :79.64%\n",
      "Epoch28.Training loss:0.012.validation loss:0.574.validaiton accuracy :79.64%\n",
      "Epoch29.Training loss:0.014.validation loss:0.576.validaiton accuracy :79.64%\n",
      "Epoch31.Training loss:0.013.validation loss:0.578.validaiton accuracy :79.42%\n",
      "Epoch32.Training loss:0.013.validation loss:0.581.validaiton accuracy :79.42%\n",
      "Epoch33.Training loss:0.011.validation loss:0.584.validaiton accuracy :79.42%\n",
      "Epoch34.Training loss:0.013.validation loss:0.587.validaiton accuracy :79.42%\n",
      "Epoch35.Training loss:0.013.validation loss:0.590.validaiton accuracy :79.42%\n",
      "Epoch36.Training loss:0.013.validation loss:0.593.validaiton accuracy :79.42%\n",
      "Epoch37.Training loss:0.013.validation loss:0.595.validaiton accuracy :79.42%\n",
      "Epoch38.Training loss:0.017.validation loss:0.598.validaiton accuracy :79.42%\n",
      "Epoch39.Training loss:0.013.validation loss:0.601.validaiton accuracy :79.42%\n",
      "Epoch40.Training loss:0.013.validation loss:0.604.validaiton accuracy :79.42%\n",
      "Epoch41.Training loss:0.014.validation loss:0.607.validaiton accuracy :79.42%\n",
      "Epoch42.Training loss:0.013.validation loss:0.609.validaiton accuracy :79.42%\n",
      "Epoch43.Training loss:0.011.validation loss:0.612.validaiton accuracy :79.42%\n",
      "Epoch44.Training loss:0.010.validation loss:0.615.validaiton accuracy :79.42%\n",
      "Epoch45.Training loss:0.012.validation loss:0.618.validaiton accuracy :79.42%\n",
      "Epoch46.Training loss:0.013.validation loss:0.621.validaiton accuracy :79.42%\n",
      "Epoch47.Training loss:0.012.validation loss:0.624.validaiton accuracy :79.42%\n",
      "Epoch48.Training loss:0.013.validation loss:0.627.validaiton accuracy :79.42%\n",
      "Epoch49.Training loss:0.013.validation loss:0.630.validaiton accuracy :79.42%\n",
      "Epoch50.Training loss:0.013.validation loss:0.633.validaiton accuracy :79.42%\n",
      "End of training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayush/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "input_size=10\n",
    "output_size=2\n",
    "hidden_layer_size=100\n",
    "tf.reset_default_graph()\n",
    "\n",
    "inputs=tf.placeholder(tf.float32,[None,input_size])\n",
    "targets=tf.placeholder(tf.int32,[None,output_size])\n",
    "\n",
    "weights_1=tf.get_variable(\"weights_1\",[input_size,hidden_layer_size])\n",
    "biases_1=tf.get_variable(\"biases_1\",[hidden_layer_size])\n",
    "\n",
    "outputs_1=tf.nn.relu(tf.matmul(inputs,weights_1)+biases_1)\n",
    "\n",
    "weights_2=tf.get_variable(\"weights_2\",[hidden_layer_size,hidden_layer_size])\n",
    "biases_2=tf.get_variable(\"biases_2\",[hidden_layer_size])\n",
    "\n",
    "outputs_2=tf.nn.relu(tf.matmul(outputs_1,weights_2)+biases_2)\n",
    "\n",
    "weights_3=tf.get_variable(\"weights_3\",[hidden_layer_size,output_size])\n",
    "biases_3=tf.get_variable(\"biases_3\",[output_size])\n",
    "\n",
    "outputs=tf.matmul(outputs_2,weights_3)+biases_3\n",
    "\n",
    "loss=tf.nn.softmax_cross_entropy_with_logits(logits=outputs,labels =targets)\n",
    "mean_loss=tf.reduce_mean(loss)\n",
    "\n",
    "optimize=tf.train.AdamOptimizer(learning_rate=0.001).minimize(mean_loss)\n",
    "\n",
    "out_equals_target=tf.equal(tf.argmax(outputs,1),tf.argmax(targets,1 ))\n",
    "\n",
    "accuracy=tf.reduce_mean(tf.cast(out_equals_target,tf.float32))\n",
    "\n",
    "sess=tf.InteractiveSession()\n",
    "initializer=tf.global_variables_initializer()\n",
    "sess.run(initializer)\n",
    "\n",
    "batch_size=100\n",
    "max_epochs=50\n",
    "prev_validation_loss=99999999.\n",
    "\n",
    "train_data=Audiobooks_Data_Reader('train',batch_size)\n",
    "validation_data=Audiobooks_Data_Reader('validation')\n",
    "\n",
    "for epoch_counter in range(max_epochs):\n",
    "    curr_epoch_loss=0.\n",
    "    for input_batch,target_batch in train_data:\n",
    "        _,batch_loss=sess.run([optimize,mean_loss],\n",
    "                             feed_dict={inputs:input_batch,targets:target_batch})\n",
    "        \n",
    "        curr_epoch_loss +=batch_loss\n",
    "        \n",
    "        curr_epoch_loss /=train_data.batch_count\n",
    "        \n",
    "        validation_loss=0\n",
    "        validation_accuracy=0\n",
    "        \n",
    "        for input_batch,target_batch in validation_data:\n",
    "            validation_loss,validation_accuracy=sess.run([mean_loss,accuracy],\n",
    "                                                        feed_dict={inputs:input_batch,targets:target_batch})\n",
    "            \n",
    "        print('Epoch'+str(epoch_counter+1)+\n",
    "             '.Training loss:'+'{0:.3f}'.format(curr_epoch_loss)+\n",
    "             '.validation loss:'+'{0:.3f}'.format(validation_loss)+\n",
    "             '.validaiton accuracy :'+'{0:.2f}'.format(validation_accuracy*100.)+'%')\n",
    "    \n",
    "        if validation_loss > prev_validation_loss:\n",
    "            break\n",
    "        prev_validation_loss=validation_loss\n",
    "        \n",
    "print(\"End of training\")\n",
    "             \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:79.42%\n"
     ]
    }
   ],
   "source": [
    "#testing the model.\n",
    "\n",
    "test_data=Audiobooks_Data_Reader('test')\n",
    "\n",
    "for input_batch,target_batch in test_data:\n",
    "    test_accuracy=sess.run([accuracy],\n",
    "                          feed_dict={inputs:input_batch,targets:target_batch})\n",
    "    test_accuracy_percent=test_accuracy[0]*100.\n",
    "    print('test accuracy:'+'{0:.2f}'.format(test_accuracy_percent)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
